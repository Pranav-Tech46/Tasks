{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import pickle\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'G6KG9OYXpbAYiFE175JngbQ23nUBTbHj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles(articles):\n",
    "    years_num = list(np.arange(1981,2019))\n",
    "    years = []\n",
    "\n",
    "    for year in years_num:\n",
    "        years.append(str(year))\n",
    "\n",
    "    months_num = list(np.arange(1,13))\n",
    "    months = []\n",
    "    daysIn = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\n",
    "    for month in months_num:\n",
    "        months.append(str(month))\n",
    "    for year in years:\n",
    "        articles[year] = {}\n",
    "        for month in months:\n",
    "            if year == '2019' and month == '5':\n",
    "                break\n",
    "            articles[year][month] = {}\n",
    "            print('Getting {}/{} articles'.format(year,month))\n",
    "            #url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "            url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'.format(year,month)\n",
    "\n",
    "            # begin = \"\"\n",
    "            # if len(month) == 1:\n",
    "            #     month = \"0\" + month\n",
    "            # begin += year + month + \"01\"\n",
    "            # endDay = str(daysIn[int(month) - 1])\n",
    "            # end = year + month + endDay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #params = {'begin_date': begin, 'end_date': end ,'api-key': api_key}\n",
    "            params = {'api-key': api_key}\n",
    "            response = requests.get(url, params=params)\n",
    "            print(response)\n",
    "\n",
    "            while response.status_code != 200:\n",
    "                print('trying again...')\n",
    "                time.sleep(3)\n",
    "                response = requests.get(url, params=params)\n",
    "                print('status code: {}'.format(response.status_code))\n",
    "\n",
    "            articlesjson = json.loads(response.text)\n",
    "            print(articlesjson)\n",
    "            return\n",
    "            docs = articlesjson['response']['docs']\n",
    "\n",
    "            articles[year][month] = len(docs)\n",
    "\n",
    "        print('Dumping year {}.'.format(year))\n",
    "\n",
    "        with open('archive_export.json', 'w') as fp:\n",
    "            json.dump(articles, fp)\n",
    "\n",
    "    print('Exporting articles 1981_1-2018_2 to json')\n",
    "    with open('archive_export.json', 'w') as fp:\n",
    "        json.dump(articles, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    begin_dates = [20000101, 20010101, 20020101]\n",
    "    end_dates = [20001231, 20011231, 20021231]\n",
    "    begin_date = 19930101\n",
    "    end_date = 19931231\n",
    "    index = 1\n",
    "    articles = {}\n",
    "    exportno = 5\n",
    "    page = 1\n",
    "\n",
    "    while page < 101 and exportno < 50 and index <= 3:\n",
    "        response = None\n",
    "        if len(articles)>=1000:\n",
    "            print('Exporting results till page {} to json {}'.format(page-1,exportno))\n",
    "            with open('metadata_export{}.json'.format(exportno), 'w') as fp:\n",
    "                json.dump(articles, fp)\n",
    "            articles = {}\n",
    "            exportno += 1\n",
    "\n",
    "\n",
    "#         print('Getting articles from page {}'.format(page))\n",
    "\n",
    "        #Foreign, Business\n",
    "        url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "        news = \"section_name:(\\\"Your Money\\\" \\\"Job Market\\\" \\\"Business\\\" \\\"World\\\" \\\"Business Day\\\" \\\"Technology\\\") AND document_type:(\\\"article\\\") AND type_of_material:(\\\"news\\\")\"\n",
    "\n",
    "        tokens = \"Job Market\"\n",
    "        params = {'api-key': api_key,\n",
    "                  'begin_date': begin_date,\n",
    "                  'end_date': end_date,\n",
    "                  'fq': news,\n",
    "                  'page': page}\n",
    "\n",
    "        while response == None:\n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                break\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "\n",
    "#         print('status code: {}'.format(response.status_code))\n",
    "\n",
    "        time.sleep(0.6)\n",
    "        temp = False\n",
    "        while temp or response.status_code != 200:\n",
    "            print('trying again...')\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                if response.status_code == 200:\n",
    "                    break\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                temp = True\n",
    "                continue\n",
    "\n",
    "        articlesjson = json.loads(response.text)\n",
    "\n",
    "        docs = articlesjson['response']['docs']\n",
    "\n",
    "        try:\n",
    "            for i in range(0,len(docs)):\n",
    "                item = docs[i]\n",
    "\n",
    "                # Get URL\n",
    "                articles[item['web_url']] = {}\n",
    "\n",
    "                # Get Name of Writer\n",
    "                try:\n",
    "                    articles[item['web_url']]['writer_name'] = item['byline']['person'][0]['firstname']+' '+item['byline']['person'][0]['lastname']\n",
    "                except:\n",
    "                    articles[item['web_url']]['writer_name'] = None\n",
    "\n",
    "                # Get Publication Date\n",
    "                try:\n",
    "                    articles[item['web_url']]['pub_date'] = item['pub_date'][:10]\n",
    "                    tempdate = ''.join((item['pub_date'][:10]).split('-'))\n",
    "                except:\n",
    "                    articles[item['web_url']]['pub_date'] = None\n",
    "\n",
    "                # Get Snippet\n",
    "                try:\n",
    "                    articles[item['web_url']]['snippet'] = item['snippet']\n",
    "                except:\n",
    "                    articles[item['web_url']]['snippet'] = None\n",
    "\n",
    "                # Get Word Count\n",
    "                try:\n",
    "                    articles[item['web_url']]['word_count'] = item['word_count']\n",
    "                except:\n",
    "                    articles[item['web_url']]['word_count'] = None\n",
    "\n",
    "                 # Get Score\n",
    "                try:\n",
    "                    articles[item['web_url']]['score'] = item['score']\n",
    "                except:\n",
    "                    articles[item['web_url']]['score'] = None\n",
    "\n",
    "                # Get Source\n",
    "                try:\n",
    "                    articles[item['web_url']]['source'] = item['source']\n",
    "                except:\n",
    "                    articles[item['web_url']]['source'] = None\n",
    "\n",
    "                # Get Section Name\n",
    "                try:\n",
    "                    articles[item['web_url']]['section_name'] = item['section_name']\n",
    "                except:\n",
    "                    articles[item['web_url']]['section_name'] = None\n",
    "\n",
    "                # Get Type of Material\n",
    "                try:\n",
    "                    articles[item['web_url']]['type_of_material'] = item['type_of_material']\n",
    "                except:\n",
    "                    articles[item['web_url']]['type_of_material'] = None\n",
    "\n",
    "                # Get Document Type\n",
    "                try:\n",
    "                    articles[item['web_url']]['document_type'] = item['document_type']\n",
    "                except:\n",
    "                    articles[item['web_url']]['document_type'] = None\n",
    "\n",
    "                # Get Main / Web Headline\n",
    "                try:\n",
    "                    articles[item['web_url']]['main_headline'] = item['headline']['main']\n",
    "                except:\n",
    "                    articles[item['web_url']]['main_headline'] = None\n",
    "\n",
    "                # Get Print Headline\n",
    "                try:\n",
    "                    articles[item['web_url']]['print_headline'] = item['headline']['print_headline']\n",
    "                except:\n",
    "                    articles[item['web_url']]['print_headline'] = None\n",
    "\n",
    "            page += 1\n",
    "\n",
    "            if page%10==0 and page>0:\n",
    "                print('Get articles page {} success'.format(page))\n",
    "\n",
    "        except Exception as e:\n",
    "            print('ERROR:, {}'.format(e))\n",
    "            return {}\n",
    "\n",
    "        # if page == 200:\n",
    "        #     end_date = end_dates[index]\n",
    "        #     begin_date = begin_dates[index]\n",
    "        #     index += 1\n",
    "        #     page = 0\n",
    "        #     break\n",
    "\n",
    "    print('Exporting remainder including page {} to json {}'.format(page,exportno))\n",
    "    with open('metadata_export{}.json'.format(exportno), 'w') as fp:\n",
    "        json.dump(articles, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bodytext():\n",
    "    #arr = [\"metadata_1981_2\", \"metadata_1982_1\", \"metadata_1982_2\", \"metadata_1982_3\", \"metadata_1983_1\", \"metadata_1983_2\", \"metadata_1983_3\", \"metadata_1984_1\", \"metadata_1984_2\", \"metadata_1984_3\", \"metadata_1985_1\", \"metadata_1985_2\", \"metadata_1986_1\", \"metadata_1986_2\", \"metadata_1986_3\", \"metadata_1987_1\", \"metadata_1987_2\", \"metadata_1987_3\", \"metadata_1988_1\", \"metadata_1988_2\", \"metadata_1988_3\", \"metadata_1989_1\", \"metadata_1989_2\", \"metadata_1989_3\", \"metadata_1990_1\", \"metadata_1990_2\", \"metadata_1990_3\", \"metadata_1991_1\", \"metadata_1991_2\", \"metadata_1992_1\", \"metadata_1992_2\", \"metadata_1993_1\", \"metadata_1993_2\", \"metadata_1994_1\", \"metadata_1994_2\", \"metadata_1995_1\", \"metadata_1995_2\", \"metadata_1996_1\", \"metadata_1996_2\", \"metadata_1997_1\", \"metadata_1997_2\", \"metadata_1998_1\", \"metadata_1998_2\", \"metadata_1999_1\", \"metadata_1999_2\", \"metadata_2000_1\", \"metadata_2000_2\", \"metadata_2001_1\", \"metadata_2001_2\", \"metadata_2002_1\", \"metadata_2002_2\"]\n",
    "    arr = [\"metadata_2017_1\", \"metadata_2017_2\", \"metadata_2018_1\", \"metadata_2018_2\"]\n",
    "    i = 0\n",
    "    for file in arr:\n",
    "        name = \"Temp/\"\n",
    "        name += file\n",
    "        name += \".json\"\n",
    "        with open(name) as fp:\n",
    "            articles = json.load(fp)\n",
    "\n",
    "        for url in articles.keys():\n",
    "            print(i)\n",
    "            i += 1\n",
    "#             print(url)\n",
    "            page = None\n",
    "            while page == None:\n",
    "                try:\n",
    "                    #page = requests.get(url, timeout=9)\n",
    "                    page = requests.get(url)\n",
    "                    break\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                except requests.exceptions.Timeout:\n",
    "                    print(\"not good\")\n",
    "                    time.sleep(45)\n",
    "                    continue\n",
    "\n",
    "#             time.sleep(0.2)\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "            text = soup.findAll(attrs={'class':'story-body-text story-content'})\n",
    "            if text == []:\n",
    "                text = soup.findAll(attrs={'class':'story-body-text'})\n",
    "            if text == []:\n",
    "                text = soup.findAll(attrs={'itemprop':'articleBody'})\n",
    "            if text == []:\n",
    "                text = soup.findAll(attrs={'itemprop':'reviewBody'})\n",
    "            body_text = ''\n",
    "            for paragraph in text:\n",
    "                body_text += (' **********'+paragraph.get_text())\n",
    "            articles[url]['body_text'] = body_text\n",
    "\n",
    "\n",
    "\n",
    "#             time.sleep(0.6)\n",
    "        # Write to .json\n",
    "        print('{}: exporting to bodytext_export{}.json'.format(datetime.datetime.now(),i))\n",
    "        output = \"FullOutput/\"\n",
    "        output += file[9:]\n",
    "        output += \"_fulltext\"\n",
    "        output += \".json\"\n",
    "        with open(output, 'w') as fp:\n",
    "            json.dump(articles, fp)\n",
    "\n",
    "    print(\"end\")\n",
    "    print(i)\n",
    "    print(\"articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get articles page 10 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 20 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 30 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 40 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 50 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 60 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 70 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 80 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 90 success\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "trying again...\n",
      "Get articles page 100 success\n",
      "Exporting remainder including page 101 to json 5\n"
     ]
    }
   ],
   "source": [
    "if 'darwin' in sys.platform:\n",
    "    print('Running \\'caffeinate\\' on MacOSX to prevent the system from sleeping')\n",
    "    subprocess.Popen('caffeinate')\n",
    "get_metadata()\n",
    "#get_bodytext()\n",
    "#get_all_articles(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e34895d9f1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_all_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "get_all_articles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
